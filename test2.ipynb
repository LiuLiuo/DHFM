{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenHowNet succeeded!\n"
     ]
    }
   ],
   "source": [
    "import OpenHowNet\n",
    "hownet_dict = OpenHowNet.HowNetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sememes = hownet_dict.get_all_sememes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sememes = []\n",
    "for se in all_sememes:\n",
    "    sememes.append(str(se).split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'portable',\n",
       " 'enclosure',\n",
       " 'in',\n",
       " 'which',\n",
       " 'baby',\n",
       " 'may',\n",
       " 'be',\n",
       " 'left',\n",
       " 'to',\n",
       " 'play']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'an enclosure for confine livestock'\n",
    "a = a.split(' ')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "left\n"
     ]
    }
   ],
   "source": [
    "for word in a:\n",
    "    if word in sememes:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[No.222929|pen|笔杆子,\n",
       " No.201533|pen|牢,\n",
       " No.222881|pen|笔,\n",
       " No.52968|pen|写,\n",
       " No.166428|pen|栏,\n",
       " No.222927|pen|笔杆,\n",
       " No.279037|pen|钢笔,\n",
       " No.85451|pen|围栏,\n",
       " No.86528|pen|圈,\n",
       " No.166402|pen|栈,\n",
       " No.70674|pen|厩,\n",
       " No.202194|pen|牿]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hownet_dict.get_sense(\"pen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenHowNet succeeded!\n",
      "Initializing BabelNet Synset Dict succeeded!\n"
     ]
    }
   ],
   "source": [
    "hownet_dict_advanced = OpenHowNet.HowNetDict(init_babel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_list = hownet_dict_advanced.get_synset('pen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[bn:00061363n|penicillin|青霉素, bn:00085489v|compose|创作, bn:00061314n|pen|笔]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sense': No.52968|pen|写, 'sememes': [compile|编辑]},\n",
       " {'sense': No.70674|pen|厩,\n",
       "  'sememes': [livestock|牲畜, facilities|设施, foster|饲养]},\n",
       " {'sense': No.85451|pen|围栏,\n",
       "  'sememes': [livestock|牲畜, facilities|设施, foster|饲养]},\n",
       " {'sense': No.86528|pen|圈,\n",
       "  'sememes': [livestock|牲畜, facilities|设施, foster|饲养]},\n",
       " {'sense': No.166402|pen|栈,\n",
       "  'sememes': [livestock|牲畜, facilities|设施, foster|饲养]},\n",
       " {'sense': No.166428|pen|栏,\n",
       "  'sememes': [livestock|牲畜, facilities|设施, foster|饲养]},\n",
       " {'sense': No.201533|pen|牢,\n",
       "  'sememes': [livestock|牲畜, facilities|设施, foster|饲养]},\n",
       " {'sense': No.202194|pen|牿,\n",
       "  'sememes': [livestock|牲畜, facilities|设施, foster|饲养]},\n",
       " {'sense': No.222881|pen|笔, 'sememes': [tool|用具, write|写, draw|画]},\n",
       " {'sense': No.222927|pen|笔杆, 'sememes': [tool|用具, write|写, draw|画]},\n",
       " {'sense': No.222929|pen|笔杆子, 'sememes': [tool|用具, write|写, draw|画]},\n",
       " {'sense': No.279037|pen|钢笔, 'sememes': [tool|用具, write|写, draw|画]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hownet_dict.get_sememes_by_word(word = 'pen', display='list', merge=False, expanded_layer=-1, K=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a correctional institution for those convicted of major crimes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma_from_key(\"pen%1:06:02::\").synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('apple.n.01'): fruit with red or yellow or green skin and sweet to tart crisp whitish flesh\n",
      "Synset('apple.n.02'): native Eurasian tree widely cultivated in many varieties for its firm rounded edible fruits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word = \"apple\"\n",
    "\n",
    "# Get synsets for the word\n",
    "synsets = wn.synsets(word)\n",
    "\n",
    "# Loop through each synset\n",
    "for synset in synsets:\n",
    "    # Print the synset and its definition\n",
    "    print(f\"{synset}: {synset.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148730\n"
     ]
    }
   ],
   "source": [
    "synsets = wn.all_synsets()\n",
    "\n",
    "# Initialize a set to hold unique words\n",
    "words = set()\n",
    "\n",
    "# Loop through each synset\n",
    "for synset in synsets:\n",
    "    # Add each lemma name (word) to the set\n",
    "    for lemma in synset.lemmas():\n",
    "        words.add(lemma.name())\n",
    "\n",
    "# Print the number of unique words\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwsd_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/SACE-main/wsd_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from wsd_models.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liujiaheng/.conda/envs/wsd/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at OFA-Sys/chinese-clip-vit-base-patch16 were not used when initializing ChineseCLIPVisionModel: ['text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.self.key.weight', 'visual_projection.weight', 'text_model.encoder.layer.10.intermediate.dense.weight', 'logit_scale', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_projection.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.embeddings.token_type_embeddings.weight', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.weight']\n",
      "- This IS expected if you are initializing ChineseCLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ChineseCLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'OFA-Sys/chinese-clip-vit-base-patch16'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'OFA-Sys/chinese-clip-vit-base-patch16' is the correct path to a directory containing all relevant files for a CLIPTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m CLIPProcessor, ChineseCLIPVisionModel\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m ChineseCLIPVisionModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mOFA-Sys/chinese-clip-vit-base-patch16\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m processor \u001b[39m=\u001b[39m CLIPProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mOFA-Sys/chinese-clip-vit-base-patch16\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/wsd/lib/python3.8/site-packages/transformers/processing_utils.py:184\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    155\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m    Instantiate a processor associated with a pretrained model.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m            [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.conda/envs/wsd/lib/python3.8/site-packages/transformers/processing_utils.py:228\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m         attribute_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m--> 228\u001b[0m     args\u001b[39m.\u001b[39mappend(attribute_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/.conda/envs/wsd/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1809\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1804\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1805\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1806\u001b[0m     )\n\u001b[1;32m   1808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1809\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1810\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1811\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1812\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1813\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1814\u001b[0m     )\n\u001b[1;32m   1816\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1817\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'OFA-Sys/chinese-clip-vit-base-patch16'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'OFA-Sys/chinese-clip-vit-base-patch16' is the correct path to a directory containing all relevant files for a CLIPTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, ChineseCLIPVisionModel\n",
    "\n",
    "model = ChineseCLIPVisionModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.1609, -0.6983],\n",
      "          [-0.6131, -0.4174]],\n",
      "\n",
      "         [[ 1.0002,  0.6214],\n",
      "          [-0.3215,  0.0833]],\n",
      "\n",
      "         [[ 1.8233,  0.0771],\n",
      "          [-0.4735,  0.8351]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9365, -2.4288],\n",
      "          [ 0.6688, -1.1741]],\n",
      "\n",
      "         [[ 1.7236,  0.8944],\n",
      "          [ 1.2349,  1.2217]],\n",
      "\n",
      "         [[ 0.6862, -1.7535],\n",
      "          [-1.5040, -0.5219]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8559, -0.5865],\n",
      "          [ 0.7369, -0.9213]],\n",
      "\n",
      "         [[-0.5456,  2.0750],\n",
      "          [ 0.2754, -0.0106]],\n",
      "\n",
      "         [[-1.0789, -0.9750],\n",
      "          [ 0.2775,  1.2507]]]])\n",
      "tensor([[[[ 1.1609, -0.6983],\n",
      "          [-0.6131, -0.4174]],\n",
      "\n",
      "         [[ 1.0002,  0.6214],\n",
      "          [-0.3215,  0.0833]],\n",
      "\n",
      "         [[ 1.8233,  0.0771],\n",
      "          [-0.4735,  0.8351]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8559, -0.5865],\n",
      "          [ 0.7369, -0.9213]],\n",
      "\n",
      "         [[-0.5456,  2.0750],\n",
      "          [ 0.2754, -0.0106]],\n",
      "\n",
      "         [[-1.0789, -0.9750],\n",
      "          [ 0.2775,  1.2507]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9365, -2.4288],\n",
      "          [ 0.6688, -1.1741]],\n",
      "\n",
      "         [[ 1.7236,  0.8944],\n",
      "          [ 1.2349,  1.2217]],\n",
      "\n",
      "         [[ 0.6862, -1.7535],\n",
      "          [-1.5040, -0.5219]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设A的值\n",
    "A = torch.randn(4, 3, 3, 2, 2)\n",
    "print(A[1])\n",
    "# 假设mask的值\n",
    "mask = torch.tensor([[0, 1, 0], [1, 0, 1], [0, 0, 1], [1, 0, 1]])\n",
    "\n",
    "sorted_indices = mask.argsort(dim=1, descending=True)\n",
    "A_sorted = torch.gather(A, 1, sorted_indices.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(-1, 3, 3, 2, 2))\n",
    "mask_sorted = torch.gather(mask, 1, sorted_indices)\n",
    "\n",
    "print(A_sorted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
